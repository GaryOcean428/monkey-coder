#!/usr/bin/env python3
"""
Use Monkey Coder to generate streaming support implementation.
"""

import asyncio
import os
from pathlib import Path
from dotenv import load_dotenv
import sys
sys.path.insert(0, 'packages/core')

from monkey_coder.core.agent_executor import AgentExecutor
from monkey_coder.providers import ProviderRegistry
from monkey_coder.filesystem import write_file

# Load environment
env_path = Path('packages/core/.env.local')
if env_path.exists():
    load_dotenv(env_path)
    print(f'Loaded {env_path}')

async def generate_streaming_support():
    """Use Monkey Coder to generate streaming implementation."""
    registry = ProviderRegistry()
    await registry.initialize_all()
    executor = AgentExecutor(registry)
    
    print("=" * 60)
    print("MONKEY CODER GENERATING STREAMING SUPPORT")
    print("=" * 60)
    
    # Generate streaming implementation
    result = await executor.execute_agent_task(
        agent_type='developer',
        prompt="""Create a complete Python implementation for Server-Sent Events (SSE) streaming in FastAPI with these requirements:

1. SSE endpoint implementation:
   - async def stream_response(request_id: str)
   - Yield formatted SSE events
   - Handle client disconnections gracefully
   - Support heartbeat/keepalive

2. Provider streaming integration:
   - Modify generate_completion to support stream=True
   - Handle OpenAI streaming response format
   - Handle Anthropic streaming format
   - Convert provider streams to SSE format

3. Event formatting:
   - Format: "data: {json}\\n\\n"
   - Event types: token, error, done
   - Include metadata: tokens used, model, provider

4. Client connection management:
   - Track active connections
   - Clean up on disconnect
   - Handle timeouts

Include all imports, error handling, and make it production-ready.
Focus on FastAPI SSE implementation using EventSourceResponse.""",
        provider='openai',
        model='gpt-4.1',
        max_tokens=4000
    )
    
    if result.get('status') == 'completed':
        code = result.get('output', '')
        
        # Extract Python code if wrapped in markdown
        if '```python' in code:
            start = code.find('```python') + 9
            end = code.find('```', start)
            if end > start:
                code = code[start:end].strip()
        
        # Save the streaming implementation
        streaming_path = Path('packages/core/monkey_coder/streaming/sse_handler.py')
        streaming_path.parent.mkdir(parents=True, exist_ok=True)
        
        write_file(str(streaming_path), code)
        print(f"\n✅ Generated streaming implementation: {streaming_path}")
        print(f"   Size: {len(code)} bytes")
        
        # Generate __init__.py
        init_content = '''"""
Streaming support for real-time AI responses.
Generated by Monkey Coder using GPT-4.1.
"""

from .sse_handler import (
    stream_response,
    format_sse_event,
    create_streaming_endpoint,
    StreamManager,
)

__all__ = [
    "stream_response",
    "format_sse_event", 
    "create_streaming_endpoint",
    "StreamManager",
]
'''
        
        init_path = streaming_path.parent / '__init__.py'
        write_file(str(init_path), init_content)
        print(f"✅ Created: {init_path}")
        
        return code
    else:
        raise Exception(f"Generation failed: {result.get('error')}")

async def main():
    try:
        code = await generate_streaming_support()
        print("\n" + "=" * 60)
        print("SUCCESS! Monkey Coder has generated streaming support!")
        print("=" * 60)
        
        # Show preview
        print("\nGenerated code preview:")
        print("-" * 40)
        lines = code.split('\n')[:30]
        for line in lines:
            print(line)
        if len(code.split('\n')) > 30:
            print("... (truncated)")
            
    except Exception as e:
        print(f"\n❌ Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())