#!/usr/bin/env python3
"""Test code generation with all providers through the local development server."""

import asyncio
import aiohttp
import json
from typing import Dict, Any
import pytest

# The development API key from the server logs
API_KEY = "mk-OtxdIS0j6FLa"
BASE_URL = "http://localhost:8000"

@pytest.mark.skip("Manual integration script; requires live server at BASE_URL and real providers.")
def test_code_generation():  # pragma: no cover - collected but skipped
    """Pytest placeholder so this file is skipped in automated suites."""
    pass

async def run_code_generation(provider: str, model: str, task: str) -> Dict[str, Any]:
    """Execute code generation request (used by manual main())."""

    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "prompt": task,
        "task_type": "code_generation",
        "context": {
            "user_id": "test_user",
            "session_id": "test_session",
            "environment": "development",
            "timeout": 60,
            "max_tokens": 2000,
            "temperature": 0.1
        },
        "persona_config": {
            "persona": "developer",
            "confidence_threshold": 0.7,
            "style_preferences": {
                "code_style": "clean",
                "documentation_level": "detailed"
            }
        },
        "orchestration_config": {
            "strategy": "simple",
            "parallel_agents": 1,
            "enable_caching": False
        },
        "provider_config": {
            "preferred_providers": [provider],
            "model_preferences": {
                provider: model
            },
            "fallback_enabled": False
        }
    }

    async with aiohttp.ClientSession() as session:
        try:
            async with session.post(
                f"{BASE_URL}/v1/execute",
                headers=headers,
                json=payload,
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                result = await response.json()
                return {
                    "provider": provider,
                    "model": model,
                    "status": response.status,
                    "success": response.status == 200,
                    "response": result
                }
        except Exception as e:
            return {
                "provider": provider,
                "model": model,
                "status": 0,
                "success": False,
                "error": str(e)
            }

async def main():
    """Test all providers with a simple code generation task."""

    # Simple task from the roadmap: Implement a REST API endpoint
    task = "Implement a REST API endpoint in Python using FastAPI that returns a list of users with pagination support. Include proper error handling and documentation."

    # Test configurations for each provider
    test_cases = [
        ("openai", "gpt-4.1"),  # Using available model
        ("anthropic", "claude-3-5-sonnet-20241022"),  # Latest Claude
        ("google", "gemini-2.5-pro"),  # Available Gemini model
        ("groq", "llama-3.3-70b-versatile"),  # Groq's Llama model
        ("xai", "grok-3"),  # xAI's Grok model
    ]

    print("=" * 80)
    print("TESTING CODE GENERATION WITH ALL PROVIDERS")
    print("=" * 80)
    print(f"\nTask: {task}\n")
    print("-" * 80)

    results = []
    for provider, model in test_cases:
        print(f"\nTesting {provider} with {model}...")
        result = await run_code_generation(provider, model, task)
        results.append(result)

        if result["success"]:
            print(f"‚úÖ {provider}: SUCCESS (Status: {result['status']})")

            # Check if we got actual code or mock response
            response_data = result["response"]
            if "result" in response_data:
                code = response_data["result"]
                # Handle both string and dict results
                if isinstance(code, dict):
                    code_str = json.dumps(code, indent=2)
                else:
                    code_str = str(code)

                # Check for common mock response patterns
                if "mock" in code_str.lower() or "generated by" in code_str.lower():
                    print("‚ö†Ô∏è  WARNING: Response appears to be MOCK data!")
                    print(f"   First 200 chars: {code_str[:200]}...")
                else:
                    print("üìù Generated code preview (first 300 chars):")
                    print(f"   {code_str[:300]}...")

                # Check token usage
                if "usage" in response_data:
                    usage = response_data["usage"]
                    if usage and usage.get("total_tokens", 0) == 0:
                        print("‚ö†Ô∏è  WARNING: Token count is 0 - likely MOCK response!")
                    elif usage:
                        print(f"üìä Tokens used: {usage.get('total_tokens', 'N/A')}")
            else:
                print("‚ùå No 'result' field in response")
                print(f"   Response: {json.dumps(response_data, indent=2)[:500]}...")
        else:
            print(f"‚ùå {provider}: FAILED")
            if "error" in result:
                print(f"   Error: {result['error']}")
            elif "response" in result:
                print(f"   Response: {json.dumps(result['response'], indent=2)[:500]}...")

    print("\n" + "=" * 80)
    print("SUMMARY")
    print("=" * 80)

    success_count = sum(1 for r in results if r["success"])
    print(f"\n‚úÖ Successful: {success_count}/{len(results)}")
    print(f"‚ùå Failed: {len(results) - success_count}/{len(results)}")

    # Check for mock responses
    mock_count = 0
    for r in results:
        if r["success"] and "result" in r.get("response", {}):
            code = r["response"]["result"]
            code_str = json.dumps(code, indent=2) if isinstance(code, dict) else str(code)
            if "mock" in code_str.lower() or "generated by" in code_str.lower():
                mock_count += 1

    if mock_count > 0:
        print(f"\n‚ö†Ô∏è  WARNING: {mock_count}/{success_count} successful responses appear to be MOCK data!")
        print("   The system is NOT making real API calls to AI providers!")

    print("\n" + "=" * 80)

    # Save results for analysis
    with open("test_results.json", "w") as f:
        json.dump(results, f, indent=2)
    print("\nDetailed results saved to: test_results.json")

if __name__ == "__main__":
    asyncio.run(main())