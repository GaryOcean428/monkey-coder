# AI Model Manifest

## Overview
This manifest documents all available AI models, their capabilities, configurations, and usage guidelines for the Monkey Coder project.

## Provider Architecture

```yaml
providers:
  primary:
    - anthropic
    - openai
    - google
    - groq
    - xai
  specialized:

    - openrouter
```

## Model Configurations

### Anthropic Models (Claude 3.5+ Only)

```yaml
anthropic:
  models:
    claude-opus-4-1-20250805:
      context_window: 200000
      max_output: 32000
      capabilities:
        - advanced_reasoning
        - code_generation
        - analysis
        - refactoring
        - documentation
        - extended_thinking
        - superior_reasoning
      cost:
        input: 0.015
        output: 0.075
      use_cases:
        - complex_reasoning
        - large_codebase_analysis
        - architectural_decisions
        - advanced_problem_solving
        - most_capable_tasks

    claude-opus-4-20250514:
      context_window: 200000
      max_output: 32000
      capabilities:
        - advanced_reasoning
        - code_generation
        - analysis
        - refactoring
        - documentation
        - extended_thinking
      cost:
        input: 0.015
        output: 0.075
      use_cases:
        - complex_reasoning
        - large_codebase_analysis
        - architectural_decisions
        - advanced_problem_solving

    claude-sonnet-4-20250514:
      context_window: 200000
      max_output: 64000
      capabilities:
        - high_performance_reasoning
        - code_generation
        - analysis
        - extended_thinking
      cost:
        input: 0.003
        output: 0.015
      use_cases:
        - complex_reasoning
        - large_codebase_analysis
        - performance_critical_tasks

    claude-3-7-sonnet-20250219:
      context_window: 200000
      max_output: 64000
      capabilities:
        - high_performance_reasoning
        - code_generation
        - analysis
        - extended_thinking
      cost:
        input: 0.003
        output: 0.015
      use_cases:
        - complex_reasoning
        - large_codebase_analysis
        - early_extended_thinking

    claude-3-5-sonnet-20241022:
      context_window: 200000
      max_output: 8192
      capabilities:
        - code_generation
        - analysis
        - refactoring
        - documentation
      cost:
        input: 0.003
        output: 0.015
      use_cases:
        - complex_reasoning
        - large_codebase_analysis
        - architectural_decisions

    claude-3-5-sonnet-20240620:
      context_window: 200000
      max_output: 8192
      capabilities:
        - code_generation
        - analysis
        - refactoring
        - documentation
      cost:
        input: 0.003
        output: 0.015
      use_cases:
        - complex_reasoning
        - large_codebase_analysis
        - architectural_decisions

    claude-3-5-haiku-20241022:
      context_window: 200000
      max_output: 8192
      capabilities:
        - code_completion
        - quick_fixes
        - simple_tasks
        - vision
      cost:
        input: 0.0008
        output: 0.004
      use_cases:
        - rapid_responses
        - simple_refactoring
        - code_suggestions
```

### OpenAI Models

```yaml
openai:
  models:
    # Reasoning Models (o1/o3/o4 Series)
    o4-mini:
      context_window: 200000
      max_output: 16384
      capabilities:
        - reasoning
        - advanced_problem_solving
        - mathematical_reasoning
        - vision
        - function_calling
        - streaming
        - structured_outputs
      cost:
        input: 0.0011
        output: 0.0044
      use_cases:
        - fast_reasoning
        - affordable_problem_solving
        - step_by_step_thinking

    o3-pro:
      context_window: 200000
      max_output: 16384
      capabilities:
        - advanced_reasoning
        - extended_thinking
        - complex_problem_solving
        - multi_step_reasoning
        - scientific_reasoning
        - vision
        - function_calling
      cost:
        input: 0.020
        output: 0.080
      use_cases:
        - most_complex_reasoning
        - scientific_analysis
        - advanced_problem_solving

    o3:
      context_window: 200000
      max_output: 16384
      capabilities:
        - reasoning
        - logical_thinking
        - mathematical_reasoning
        - code_reasoning
        - vision
        - function_calling
      cost:
        input: 0.002
        output: 0.008
      use_cases:
        - complex_problem_solving
        - analysis
        - mathematical_tasks

    o3-mini:
      context_window: 200000
      max_output: 16384
      capabilities:
        - reasoning
        - quick_thinking
        - logical_analysis
        - vision
        - function_calling
      cost:
        input: 0.0006
        output: 0.0024
      use_cases:
        - compact_reasoning
        - speed_optimized_tasks
        - efficient_analysis

    o1:
      context_window: 200000
      max_output: 32768
      capabilities:
        - reasoning
        - step_by_step_thinking
        - mathematical_reasoning
        - code_reasoning
        - scientific_analysis
        - logical_deduction
      cost:
        input: 0.015
        output: 0.060
      use_cases:
        - advanced_reasoning
        - complex_problems
        - extended_thinking

    o1-mini:
      context_window: 128000
      max_output: 16384
      capabilities:
        - reasoning
        - coding
        - mathematical_reasoning
        - stem_problem_solving
        - logical_thinking
      cost:
        input: 0.003
        output: 0.012
      use_cases:
        - fast_reasoning
        - coding_tasks
        - stem_applications

    # GPT-4.1 Family
    gpt-4.1:
      context_window: 1048576
      max_output: 16384
      capabilities:
        - text
        - vision
        - function_calling
        - streaming
        - structured_outputs
        - multimodal
        - advanced_conversation
        - code_generation
        - analysis
      cost:
        input: 0.002
        output: 0.008
      use_cases:
        - general_coding
        - complex_conversation
        - api_integration
        - structured_output

    gpt-4.1-mini:
      context_window: 1048576
      max_output: 16384
      capabilities:
        - text
        - vision
        - function_calling
        - streaming
        - structured_outputs
        - fast_response
        - cost_efficient
      cost:
        input: 0.00012
        output: 0.00048
      use_cases:
        - quick_tasks
        - cost_sensitive_applications
        - code_formatting
        - simple_analysis

    gpt-4.1-vision:
      context_window: 1048576
      max_output: 16384
      capabilities:
        - text
        - vision
        - image_analysis
        - multimodal
        - function_calling
        - streaming
        - structured_outputs
        - visual_reasoning
      cost:
        input: 0.003
        output: 0.012
      use_cases:
        - vision_tasks
        - image_analysis
        - multimodal_understanding
        - visual_reasoning
```

### Google Models (Gemini 2.5 Series)

```yaml
google:
  models:
    gemini-2.5-pro:
      context_window: 2097152
      max_output: 65536
      capabilities:
        - complex_reasoning
        - code_generation
        - mathematical_reasoning
        - scientific_analysis
        - multimodal
        - function_calling
      cost:
        input: 0.00125
        output: 0.005
      use_cases:
        - state_of_the_art_reasoning
        - complex_code_tasks
        - mathematical_problems
        - scientific_applications

    gemini-2.5-flash:
      context_window: 1048576
      max_output: 8192
      capabilities:
        - fast_reasoning
        - code_generation
        - multimodal
        - function_calling
        - streaming
      cost:
        input: 0.000075
        output: 0.0003
      use_cases:
        - price_performance_optimized
        - large_scale_tasks
        - low_latency_requirements
        - general_coding

    gemini-2.5-flash-lite:
      context_window: 1048576
      max_output: 8192
      capabilities:
        - basic_reasoning
        - code_completion
        - text_generation
        - cost_efficient
      cost:
        input: 0.00001875
        output: 0.000075
      use_cases:
        - high_throughput_tasks
        - cost_critical_applications
        - simple_code_tasks
        - text_generation

    gemini-2.0-flash:
      context_window: 1048576
      max_output: 8192
      capabilities:
        - next_generation_features
        - superior_speed
        - native_tool_use
        - multimodal
        - function_calling
        - streaming
      cost:
        input: 0.0001
        output: 0.0004
      use_cases:
        - speed_critical_tasks
        - tool_usage
        - multimodal_applications
        - real_time_interactions
```

### Groq Models (Hardware Accelerated)

```yaml
groq:
  models:
    # Llama Models
    llama-3.1-8b-instant:
      context_window: 131072
      max_output: 131072
      capabilities:
        - code_generation
        - analysis
        - fast_inference
        - streaming
      cost:
        input: 0.00005
        output: 0.00008
      use_cases:
        - rapid_prototyping
        - code_review
        - interactive_coding
        - lightweight_tasks

    llama-3.3-70b-versatile:
      context_window: 131072
      max_output: 32768
      capabilities:
        - code_generation
        - analysis
        - versatile_reasoning
        - streaming
      cost:
        input: 0.00059
        output: 0.00079
      use_cases:
        - complex_reasoning
        - code_architecture
        - detailed_analysis
        - versatile_tasks

    meta-llama/llama-4-maverick-17b-128e-instruct:
      context_window: 131072
      max_output: 8192
      capabilities:
        - advanced_reasoning
        - instruction_following
        - code_generation
        - streaming
      cost:
        input: 0.0002
        output: 0.0003
      use_cases:
        - latest_llama_capabilities
        - advanced_reasoning
        - instruction_following
        - preview_features

    meta-llama/llama-4-scout-17b-16e-instruct:
      context_window: 131072
      max_output: 8192
      capabilities:
        - reasoning
        - instruction_following
        - code_generation
        - streaming
      cost:
        input: 0.0002
        output: 0.0003
      use_cases:
        - latest_llama_capabilities
        - balanced_reasoning
        - instruction_following
        - preview_features

    # Specialized Models
    moonshotai/kimi-k2-instruct:
      context_window: 131072
      max_output: 16384
      capabilities:
        - advanced_moe_reasoning
        - code_generation
        - multilingual
        - streaming
      cost:
        input: 0.0008
        output: 0.0012
      use_cases:
        - advanced_reasoning
        - multilingual_tasks
        - complex_problem_solving
        - moe_capabilities

    qwen/qwen3-32b:
      context_window: 131072
      max_output: 40960
      capabilities:
        - advanced_reasoning
        - multilingual
        - code_generation
        - streaming
      cost:
        input: 0.0004
        output: 0.0006
      use_cases:
        - multilingual_applications
        - advanced_reasoning
        - code_generation
        - qwen_capabilities
```

### xAI Models (Grok Series)

```yaml
grok:
  models:
    grok-4-latest:
      context_window: 131072
      max_output: 131072
      capabilities:
        - reasoning
        - analysis
        - conversation
        - code_generation
      cost:
        input: 0.005
        output: 0.015
      use_cases:
        - advanced_reasoning
        - complex_analysis
        - conversational_ai
        - code_architecture


    grok-3:
      context_window: 100000
      max_output: 100000
      capabilities:
        - reasoning
        - conversation
        - code_generation
      cost:
        input: 0.002
        output: 0.008
      use_cases:
        - general_reasoning
        - conversation
        - code_analysis
        - balanced_performance

    grok-3-mini:
      context_window: 32768
      max_output: 32768
      capabilities:
        - conversation
        - basic_reasoning
      cost:
        input: 0.00025
        output: 0.001
      use_cases:
        - everyday_tasks
        - cost_efficient_operations
        - quick_responses
        - simple_coding

    grok-3-mini-fast:
      context_window: 16384
      max_output: 16384
      capabilities:
        - conversation
        - streaming
        - fast_response
      cost:
        input: 0.0001
        output: 0.0004
      use_cases:
        - ultra_fast_responses
        - simple_tasks
        - real_time_applications
        - cost_optimization

    grok-3-fast:
      context_window: 65536
      max_output: 65536
      capabilities:
        - conversation
        - streaming
        - balanced_performance
      cost:
        input: 0.001
        output: 0.004
      use_cases:
        - speed_and_capability_balance
        - interactive_coding
        - real_time_conversations
        - efficient_processing
```

### Together Models

```yaml
together:
  models:
    meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo:
      context_window: 131072
      max_output: 4096
      capabilities:
        - code_generation
        - instruction_following
        - reasoning
      cost:
        input: 0.00088
        output: 0.00088
      use_cases:
        - complex_tasks
        - code_architecture
        - detailed_analysis

    deepseek-ai/deepseek-coder-33b-instruct:
      context_window: 16384
      max_output: 4096
      capabilities:
        - specialized_coding
        - multi_language
        - code_completion
      cost:
        input: 0.00014
        output: 0.00014
      use_cases:
        - code_specific_tasks
        - language_translation
        - optimization
```

### OpenRouter Models

```yaml
openrouter:
  models:
    anthropic/claude-3.5-sonnet:
      context_window: 200000
      max_output: 8192
      capabilities:
        - premium_reasoning
        - code_generation
        - analysis
      cost:
        input: 0.003
        output: 0.015
      use_cases:
        - fallback_provider
        - load_balancing
        - redundancy

    openai/gpt-4o:
      context_window: 128000
      max_output: 16384
      capabilities:
        - multimodal
        - function_calling
        - code_generation
      cost:
        input: 0.0025
        output: 0.01
      use_cases:
        - alternative_routing
        - availability_guarantee
        - cost_optimization
```

## Model Selection Strategy

### Task-Based Selection

```yaml
task_routing:
  complex_reasoning:
    primary: claude-opus-4-1-20250805
    fallback: o3-pro
    alternatives: [claude-opus-4-20250514, gemini-2.5-pro, o3]

  advanced_reasoning:
    primary: o3
    fallback: claude-sonnet-4-20250514
    alternatives: [gpt-4.1, gemini-2.5-pro]

  fast_reasoning:
    primary: o4-mini
    fallback: o3-mini
    alternatives: [claude-3-5-haiku-20241022, gemini-2.5-flash]

  general_coding:
    primary: gpt-4.1
    fallback: claude-3-5-sonnet-20241022
    alternatives: [gemini-2.5-flash, llama-3.3-70b-versatile]

  cost_efficient:
    primary: gpt-4.1-mini
    fallback: gemini-2.5-flash-lite
    alternatives: [claude-3-5-haiku-20241022, llama-3.1-8b-instant]

  vision_tasks:
    primary: gpt-4.1-vision
    fallback: gemini-2.0-flash
    alternatives: [claude-3-5-sonnet-20241022]

  rapid_prototyping:
    primary: llama-3.1-8b-instant
    fallback: gemini-2.5-flash
    alternatives: [gpt-4.1-mini, claude-3-5-haiku-20241022]

  multilingual:
    primary: qwen/qwen3-32b
    fallback: gemini-2.5-pro
    alternatives: [claude-sonnet-4-20250514, gpt-4.1]

  scientific_math:
    primary: o3-pro
    fallback: gemini-2.5-pro
    alternatives: [o3, claude-opus-4-20250514]

```

### Cost Optimization Rules

```yaml
cost_tiers:
  budget:
    models:
      - gpt-4.1-mini
      - gemini-2.5-flash-lite
      - claude-3-5-haiku-20241022
      - llama-3.1-8b-instant
    max_cost_per_1k_tokens: 0.0005

  balanced:
    models:
      - o4-mini
      - gemini-2.5-flash
      - gpt-4.1
      - llama-3.3-70b-versatile
    max_cost_per_1k_tokens: 0.005

  premium:
    models:
      - o3-pro
      - claude-opus-4-20250514
      - gemini-2.5-pro
      - o3
    max_cost_per_1k_tokens: 0.020

  ultra_premium:
    models:
      - claude-opus-4-1-20250805
      - o3-pro
      - claude-opus-4-20250514
    max_cost_per_1k_tokens: 0.100
```

## Provider-Specific Features

### Anthropic
- **Extended Thinking**: Advanced reasoning capabilities in Claude 4 series
- **Artifacts**: Structured output for code blocks
- **System Prompts**: Enhanced instruction following
- **XML Tags**: Better structured responses
- **Model Validation**: Only Claude 3.5+ models supported

### OpenAI
- **Reasoning Models**: o1/o3/o4 series with advanced thinking capabilities
- **Function Calling**: Native tool use across all models
- **JSON Mode**: Guaranteed valid JSON
- **Vision**: Image understanding for diagrams
- **Streaming**: Real-time response generation
- **Structured Outputs**: Guaranteed output formats

### Google
- **Gemini 2.5**: State-of-the-art reasoning capabilities
- **Native Tool Use**: Built-in function calling
- **Multimodal**: Advanced image and video understanding
- **Large Context**: Up to 2M tokens context window
- **Cost Efficiency**: Highly competitive pricing structure

### Groq
- **Speed**: Ultra-fast hardware-accelerated inference
- **Streaming**: Low-latency responses
- **Batch Processing**: Efficient bulk operations
- **Llama 4**: Latest preview models available
- **Specialized Models**: Kimi (MoE) and Qwen support

### Together
- **Custom Models**: Fine-tuned options
- **Inference Optimization**: Hardware acceleration
- **Model Mixing**: Ensemble approaches

### OpenRouter
- **Unified API**: Single interface for multiple providers
- **Automatic Fallback**: Built-in redundancy
- **Usage Analytics**: Cross-provider insights

## Integration Guidelines

### Environment Variables

```bash
# Required for each provider
ANTHROPIC_API_KEY=your_key
OPENAI_API_KEY=your_key
GOOGLE_API_KEY=your_key
GROQ_API_KEY=your_key
TOGETHER_API_KEY=your_key
OPENROUTER_API_KEY=your_key

# Optional configuration
DEFAULT_MODEL=gpt-4.1
FALLBACK_MODEL=claude-3-5-sonnet-20241022
MAX_RETRIES=3
TIMEOUT_SECONDS=30
REASONING_TIMEOUT=120
```

### Error Handling

```typescript
interface ModelError {
  provider: string;
  model: string;
  error: string;
  fallback_attempted: boolean;
  timestamp: string;
}

const errorStrategies = {
  rate_limit: "switch_provider",
  timeout: "retry_with_backoff",
  invalid_response: "use_fallback_model",
  provider_down: "use_alternative_provider",
  model_deprecated: "use_compliant_replacement"
};
```

## Performance Benchmarks

```yaml
benchmarks:
  response_time:
    groq: 0.3s
    gemini-2.5-flash: 0.8s
    together: 1.2s
    openai: 1.5s
    anthropic: 2.0s

  tokens_per_second:
    groq: 800
    gemini-2.5-flash: 500
    openai: 120
    anthropic: 100
    together: 200

  reasoning_quality:
    claude-opus-4-1-20250805: 9.9/10
    o3-pro: 9.8/10
    claude-opus-4-20250514: 9.7/10
    gemini-2.5-pro: 9.6/10
    o3: 9.5/10
    gpt-4.1: 9.2/10

  reliability:
    openai: 99.9%
    anthropic: 99.8%
    google: 99.8%
    openrouter: 99.7%
    groq: 99.5%
    together: 99.3%
```

## Usage Examples

### Basic Usage

```typescript
const response = await provider.complete({
  model: "gpt-4.1",
  messages: [
    { role: "user", content: "Generate a React component" }
  ],
  temperature: 0.7
});
```

### With Fallback

```typescript
const response = await provider.completeWithFallback({
  primary: "o3-pro",
  fallback: "claude-opus-4-20250514",
  messages: [...],
  maxRetries: 3
});
```

### Cost-Aware Selection

```typescript
const model = await selectModel({
  task: "complex_reasoning",
  maxCost: 0.01,
  contextLength: 50000,
  responseLength: 2000,
  capabilities: ["reasoning", "code_generation"]
});
```

### Reasoning Model Usage

```typescript
const response = await provider.complete({
  model: "o3",
  messages: [...],
  reasoning_patience: true,
  max_completion_tokens: 16384
  // Note: temperature, top_p not supported for reasoning models
});
```

## Model Compliance System

### Automatic Replacements

```typescript
const modelReplacements = {
  // OpenAI legacy models
  "gpt-4o": "gpt-4.1",
  "gpt-4o-mini": "gpt-4.1-mini",
  "gpt-4": "gpt-4.1",
  "gpt-4-turbo": "gpt-4.1",

  // Anthropic legacy models
  "claude-3-opus": "claude-opus-4-1-20250805",
  "claude-opus-4-20250514": "claude-opus-4-1-20250805",
  "claude-3-sonnet": "claude-3-5-sonnet-20241022",
  "claude-3-haiku": "claude-3-5-haiku-20241022",

  // Google legacy models
  "gemini-1.5-pro": "gemini-2.5-pro",
  "gemini-1.5-flash": "gemini-2.5-flash"
};
```

### Validation Decorator

```python
@enforce_model_compliance
async def generate_code(model="claude-3-opus"):  # Automatically converts to claude-opus-4-1-20250805
    pass
```

## Maintenance Notes

- Model versions are updated quarterly
- Cost data refreshed monthly
- Performance benchmarks run weekly
- New models evaluated within 48 hours of release
- Deprecated models supported for 90 days
- Compliance validation performed on every API call

## Model Statistics

- **Total Available Models**: 32 across 5 primary providers
- **Reasoning Models**: 6 (OpenAI o1/o3/o4 series)
- **Chat Models**: 18 (OpenAI GPT-4.1 family + Anthropic Claude + xAI Grok + others)
- **Multimodal Models**: 4 (Vision-capable models)
- **Hardware Accelerated**: 6 (Groq models)
- **Cost-Optimized**: 14 (Models under $0.001 per 1K tokens)

## Contact

For model-specific questions or issues:
- GitHub Issues: [monkey-coder/issues](https://github.com/GaryOcean428/monkey-coder/issues)
- Model Selection: Use `claude-opus-4-1-20250805` for complex decisions, `gpt-4.1` for general tasks
- Performance Issues: Check Groq models first for speed-critical tasks
- Cost Optimization: Use `gpt-4.1-mini` or `gemini-2.5-flash-lite` for budget constraints
- Compliance: All models automatically validated against approved list
